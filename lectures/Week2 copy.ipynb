{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "As explained in the [Before week 1: How to take this class](https://nbviewer.org/github/suneman/socialdata2023/blob/main/lectures/How_To_Take_This_Class.ipynb) notebook, each week of this class is an Jupyter notebook like this one. In order to follow the class, you simply start reading from the top, following the instructions.\n",
    "\n",
    "**New Info**: Remember that this week is also the time to learn a bit about how the the assignments and the final project work. So if you havn't already, check out the [Before week 2: Assignments and Final Project](https://github.com/suneman/socialdata2023/blob/main/lectures/Assignments_And_Final_Project.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Today's lecture does a few things.\n",
    "* First there is an introduction to data visualization incl a little exercise and a video (Part 1). \n",
    "* As the main event, we will work with crime-data and generate a large number of interesting and informative plots (Part 2,4,5).\n",
    "* We will also talk a bit about what makes a good plot (Part 3)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: A little visualization exercise\n",
    "\n",
    "Start by downloading these four datasets: [Data 1](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data1.tsv), [Data 2](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data2.tsv), [Data 3](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data3.tsv), and [Data 4](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data4.tsv). The format is `.tsv`, which stands for _tab separated values_. \n",
    "As you will later realize, these are famous datasets!\n",
    "Each file has two columns (separated using the tab character). The first column is $x$-values, and the second column is $y$-values.  \n",
    "\n",
    "It's ok to just download these files to disk by right-clicking on each one, but if you use Python and `urllib` or `urllib2` to get them, I'll really be impressed. If you don't know how to do that, I recommend opening up Google and typing \"download file using Python\" or something like that. When interpreting the search results remember that _stackoverflow_ is your friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "4 columns passed, passed data had 1 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\anned\\anaconda3\\envs\\data_vis\\lib\\site-packages\\pandas\\core\\internals\\construction.py:969\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anned\\anaconda3\\envs\\data_vis\\lib\\site-packages\\pandas\\core\\internals\\construction.py:1017\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m   1018\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m \u001b[39melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 4 columns passed, passed data had 1 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mwith\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(req) \u001b[39mas\u001b[39;00m response:\n\u001b[0;32m     23\u001b[0m    \u001b[39m#the_page = response.read()\u001b[39;00m\n\u001b[0;32m     24\u001b[0m    data4 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(response, delimiter \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m, header \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 26\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame([[data1],[data2],[data3],[data4]], columns \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mdata 1\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdata 2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdata 3\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdata 4\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\anned\\anaconda3\\envs\\data_vis\\lib\\site-packages\\pandas\\core\\frame.py:746\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 746\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    747\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    748\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    749\u001b[0m         data,\n\u001b[0;32m    750\u001b[0m         columns,\n\u001b[0;32m    751\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    752\u001b[0m         dtype,\n\u001b[0;32m    753\u001b[0m     )\n\u001b[0;32m    754\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    755\u001b[0m         arrays,\n\u001b[0;32m    756\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    759\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    760\u001b[0m     )\n\u001b[0;32m    761\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\anned\\anaconda3\\envs\\data_vis\\lib\\site-packages\\pandas\\core\\internals\\construction.py:510\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[1;32m--> 510\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    511\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    513\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\anned\\anaconda3\\envs\\data_vis\\lib\\site-packages\\pandas\\core\\internals\\construction.py:875\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    872\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[0;32m    873\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 875\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    876\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mc:\\Users\\anned\\anaconda3\\envs\\data_vis\\lib\\site-packages\\pandas\\core\\internals\\construction.py:972\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    969\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 972\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[0;32m    975\u001b[0m     contents \u001b[39m=\u001b[39m _convert_object_array(contents, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 4 columns passed, passed data had 1 columns"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "req = urllib.request.Request('https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data1.tsv')\n",
    "with urllib.request.urlopen(req) as response:\n",
    "   #the_page = response.read()\n",
    "   data1 = pd.read_csv(response, delimiter = '\\t', header = None, names = ['x','y'])\n",
    "\n",
    "req = urllib.request.Request('https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data2.tsv')\n",
    "with urllib.request.urlopen(req) as response:\n",
    "   #the_page = response.read()\n",
    "   data2 = pd.read_csv(response, delimiter = '\\t', header = None, names = ['x','y'])\n",
    "\n",
    "req = urllib.request.Request('https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data3.tsv')\n",
    "with urllib.request.urlopen(req) as response:\n",
    "   #the_page = response.read()\n",
    "   data3 = pd.read_csv(response, delimiter = '\\t', header = None, names = ['x','y'])\n",
    "\n",
    "req = urllib.request.Request('https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data4.tsv')\n",
    "with urllib.request.urlopen(req) as response:\n",
    "   #the_page = response.read()\n",
    "   data4 = pd.read_csv(response, delimiter = '\\t', header = None, names = ['x','y'])\n",
    "\n",
    "#data = pd.DataFrame([[data1],[data2],[data3],[data4]], columns = ['data 1', 'data 2', 'data 3', 'data 4'])\n",
    "data = pd.DataFrame([[data1,data2,data3,data4]], columns = ['data 1', 'data 2', 'data 3', 'data 4'])\n",
    "print(data.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to the exercise:\n",
    "\n",
    "> *Exercise*: \n",
    "> \n",
    "> * Using the `numpy` function `mean`, calculate the mean of both $x$-values and $y$-values for each dataset. \n",
    ">      * Use python string formatting to print precisely two decimal places of these results to the output cell. Check out [this _stackoverflow_ page](http://stackoverflow.com/questions/8885663/how-to-format-a-floating-number-to-fixed-width-in-python) for help with the string formatting. \n",
    "> * Now calculate the variance for all of the various sets of $x$- and $y$-values, by using the `numpy` function `var`. Print it to three decimal places.\n",
    "> * Use `numpy` to calculate the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) between $x$- and $y$-values for all four data sets (also print to three decimal places).\n",
    "> * The next step is use _linear regression_ to fit a straight line $f(x) = a x + b$ through each dataset and report $a$ and $b$ (to two decimal places). An easy way to fit a straight line in Python is using `scipy`'s `linregress`. It works like this\n",
    "> ```\n",
    "> from scipy import stats\n",
    "> slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    ">```\n",
    "> * Comment on the results from the previous steps. What do you observe? \n",
    "> * Finally, it's time to plot the four datasets using `matplotlib.pyplot`. Use a two-by-two [`subplot`](http://matplotlib.org/examples/pylab_examples/subplot_demo.html) to put all of the plots nicely in a grid and use the same $x$ and $y$ range for all four plots. And include the linear fit in all four plots. (To get a sense of what I think the plot should look like, you can take a look at my version [here](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/anscombe.png).)\n",
    "> * Explain - in your own words - what you think my point with this exercise is (see below for tips on this).\n",
    "\n",
    "\n",
    "Get more insight in the ideas behind this exercise by reading [here](https://en.wikipedia.org/wiki/Anscombe%27s_quartet). Here you can also get an explanation of why the datasets are actually famous - I mean they have their own Wikipedia page!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data1:\n",
      "Mean of x values: 9.00 \n",
      "Mean of y values: 7.50\n",
      "Variance of x values: 11.000 \n",
      "Variance of y values: 4.127\n",
      "Pearson correlation between x and y values: 0.816\n",
      "Linear regression: a = 0.50, b = 3.00\n",
      "Data2:\n",
      "Mean of x values: 9.00 \n",
      "Mean of y values: 7.50\n",
      "Variance of x values: 11.000 \n",
      "Variance of y values: 4.128\n",
      "Pearson correlation between x and y values: 0.816\n",
      "Linear regression: a = 0.50, b = 3.00\n",
      "Data3:\n",
      "Mean of x values: 9.00 \n",
      "Mean of y values: 7.50\n",
      "Variance of x values: 11.000 \n",
      "Variance of y values: 4.123\n",
      "Pearson correlation between x and y values: 0.816\n",
      "Linear regression: a = 0.50, b = 3.00\n",
      "Data4:\n",
      "Mean of x values: 9.00 \n",
      "Mean of y values: 7.50\n",
      "Variance of x values: 11.000 \n",
      "Variance of y values: 4.123\n",
      "Pearson correlation between x and y values: 0.817\n",
      "Linear regression: a = 0.50, b = 3.00\n"
     ]
    }
   ],
   "source": [
    "mean_x1 = data1.x.mean()\n",
    "mean_y1 = data1.y.mean()\n",
    "print(\"Data1:\\nMean of x values: {:.2f} \\nMean of y values: {:.2f}\".format(mean_x1,mean_y1)) \n",
    "var_x1 = data1.x.var()\n",
    "var_y1 = data1.y.var()\n",
    "print(\"Variance of x values: {:.3f} \\nVariance of y values: {:.3f}\".format(var_x1,var_y1)) \n",
    "corr1 = np.corrcoef(data1.x, data1.y)[0,1]\n",
    "print(\"Pearson correlation between x and y values: {:.3f}\".format(corr1)) \n",
    "a1, b1, _, _, _ = stats.linregress(data1.x,data1.y)\n",
    "print(\"Linear regression: a = {:.2f}, b = {:.2f}\".format(a1,b1)) \n",
    "\n",
    "mean_x2 = data2.x.mean()\n",
    "mean_y2 = data2.y.mean()\n",
    "print(\"Data2:\\nMean of x values: {:.2f} \\nMean of y values: {:.2f}\".format(mean_x2,mean_y2)) \n",
    "var_x2 = data2.x.var()\n",
    "var_y2 = data2.y.var()\n",
    "print(\"Variance of x values: {:.3f} \\nVariance of y values: {:.3f}\".format(var_x2,var_y2)) \n",
    "corr2 = np.corrcoef(data2.x, data2.y)[0,1]\n",
    "print(\"Pearson correlation between x and y values: {:.3f}\".format(corr2)) \n",
    "a2, b2, _, _, _ = stats.linregress(data2.x,data2.y)\n",
    "print(\"Linear regression: a = {:.2f}, b = {:.2f}\".format(a2,b2)) \n",
    "\n",
    "mean_x3 = data3.x.mean()\n",
    "mean_y3 = data3.y.mean()\n",
    "print(\"Data3:\\nMean of x values: {:.2f} \\nMean of y values: {:.2f}\".format(mean_x3,mean_y3)) \n",
    "var_x3 = data3.x.var()\n",
    "var_y3 = data3.y.var()\n",
    "print(\"Variance of x values: {:.3f} \\nVariance of y values: {:.3f}\".format(var_x3,var_y3)) \n",
    "corr3 = np.corrcoef(data3.x, data3.y)[0,1]\n",
    "print(\"Pearson correlation between x and y values: {:.3f}\".format(corr3)) \n",
    "a3, b3, _, _, _ = stats.linregress(data3.x,data3.y)\n",
    "print(\"Linear regression: a = {:.2f}, b = {:.2f}\".format(a3,b3)) \n",
    "\n",
    "mean_x4 = data4.x.mean()\n",
    "mean_y4 = data4.y.mean()\n",
    "print(\"Data4:\\nMean of x values: {:.2f} \\nMean of y values: {:.2f}\".format(mean_x4,mean_y4)) \n",
    "var_x4 = data4.x.var()\n",
    "var_y4 = data4.y.var()\n",
    "print(\"Variance of x values: {:.3f} \\nVariance of y values: {:.3f}\".format(var_x4,var_y4)) \n",
    "corr4 = np.corrcoef(data4.x, data4.y)[0,1]\n",
    "print(\"Pearson correlation between x and y values: {:.3f}\".format(corr4)) \n",
    "a4, b4, _, _, _ = stats.linregress(data4.x,data4.y)\n",
    "print(\"Linear regression: a = {:.2f}, b = {:.2f}\".format(a4,b4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you get a better sense of why data visualization is an important and powerful tool, you are ready to get a small intro on the topic! Again, don't watch the video until **after** you've done exercise 1.1 \n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/9D2aI30AMhM/0.jpg)](https://www.youtube.com/watch?v=9D2aI30AMhM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Excercise:* Questions for the lecture\n",
    "> * What is the difference between *data* and *metadata*? How does that relate to the GPS tracks-example?\n",
    "> * Sune says that the human eye is a great tool for data analysis. Do you agree? Explain why/why not. Mention something that the human eye is very good at. Can you think of something that [is difficult for the human eye](http://cdn.ebaumsworld.com/mediaFiles/picture/718392/84732652.jpg). Explain why your example is difficult. \n",
    "> * Simpson's paradox is hard to explain. Come up with your own example - or find one on line.\n",
    "> * In your own words, explain the differnece between *exploratory* and *explanatory* data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing patterns in the data\n",
    "\n",
    "Visualizing data is a powerful technique that helps us exploiting the human eye, and make complex patterns easier to identify. \n",
    "\n",
    "Let's see if we can detect any interesting patterns in the big crime-data file from San Francisco you downloaded last week. We'll again only look at the focus-crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "focuscrimes = set(['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise*: More temporal patterns. Last time we plotted the development over time (how each of the focus crimes changed over time, year-by-year). Today we'll start by looking at the developments across the months, weekdays, and across the 24 hours of the day. \n",
    ">\n",
    "> **Note:** restrict yourself to the dataset of *entire years*.\n",
    ">\n",
    "> * *Weekly patterns*. Basically, we'll forget about the yearly variation and just count up what happens during each weekday. [Here's what my version looks like](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/weekdays.png). Some things make sense - for example `drunkenness` and the weekend. But there are some aspects that were surprising to me. Check out `prostitution` and mid-week behavior, for example!?\n",
    "> * *The months*. We can also check if some months are worse by counting up number of crimes in Jan, Feb, ..., Dec. Did you see any surprises there?\n",
    "> * *The 24 hour cycle*. We can also forget about weekday and simply count up the number of each crime-type that occurs in the dataset from midnight to 1am, 1am - 2am ... and so on. Again: Give me a couple of comments on what you see. \n",
    "> * *Hours of the week*. But by looking at just 24 hours, we may be missing some important trends that can be modulated by week-day, so let's also check out the 168 hours of the week. So let's see the number of each crime-type Monday night from midninght to 1am, Monday night from 1am-2am - all the way to Sunday night from 11pm to midnight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Creating nice plots\n",
    "\n",
    "Ok. There's a lot of barcharts today. We need them ... they are a fantastic tool for data exploration. But it can get monotonous, so let's take a little break to talk about something else before digging deeper with the barcharts.\n",
    "\n",
    "I want to tell you a bit about how to make nice plots. I do that in the video below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise:* Nice plots\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Exploring other types of plots for temporal data\n",
    "\n",
    "We continue our mini-break from barcharts by exploring more ways to plot temporal data.\n",
    "\n",
    "> *Exercise (extra hard):* Other cool ways to plot temoral data. This exercise is a bit more free than the previous ones. I am going to introduce three different plot-types. Then your job is to choose a part of the crime-data that you care about - and plot it using these new ways of visualizing data. \n",
    ">\n",
    ">I recommend that you choose a different part of the crime-data for each plot-type.\n",
    "> * Calendar plots. Get started on calendar plots **[here](https://calplot.readthedocs.io/en/latest/)**. There are other packages for plotting these, those are also OK to use.\n",
    "> * [Polar bar chart](https://user-images.githubusercontent.com/12328192/89272649-be76e200-d63e-11ea-97ad-fd1ba5831c89.png). Here I want you to plot a 24-hour pattern of some sort -- those work really well in radial plots (another name for polar plots) because the day  wraps around on itself. You can also try plotting data with patterns from the 168 hours of the week. There's not one super-awesome solution here, you can try using [pure matplotlib](https://matplotlib.org/stable/gallery/pie_and_polar_charts/polar_bar.html) ... [some examples here](https://www.python-graph-gallery.com/circular-barplot/) or via [plotly](https://plotly.com/python/polar-chart/) (scroll down a bit for the polar barchart).\n",
    "> * Time series. Time series is a key functionality of `Pandas`, so here I simply recommend starting by searching your favorite search engine for something like `time series` `pandas`\n",
    "\n",
    "**Note**: I added this exercise with fewer hints than usual to give you a bit of an extra challenge. Once you're done, you'll agree that it is not difficult to create these plots. What ***IS*** difficult is figuring out all the little steps you need to do to make them work. \n",
    "\n",
    "*My philosophy for data science is this*: Getting to what you want rarely seems hard once you found your way there, the difficulty comes in breaking down a hard problem into the little steps you need to take to solve your complex problem. In this class, I usually do the breaking down for you and provide you with the steps (that's how you go from nothing to creating complex visualizations of crime-data). But I also want you to learn the breaking-problems-down part. So this time I'm trying to do that by posing a slightly more high-level set of problems than usual. \n",
    "\n",
    "*My approach is always to think*: Even if my task seems impossible, I think is there any problem that I ***CAN*** solve that will get me closer to where I want to go. Once I've solved that part, I'm smarter and I try to think: Is there a new problem I can solve that'll get me closer knowing what I know now. And I just keep going. Usually that's enough.\n",
    "\n",
    "*If that seems too abstract*, a useful goal for you is to use your internet searching skills to figure out how to make each visualization work -- look for examples, tutorials, stack overflow posts, people who have found the same error messages as you, etc, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Back to visualizing patterns in the data\n",
    "\n",
    "The next thing we'll be looking into is how crimes break down across the 10 districts in San Francisco.\n",
    "\n",
    "> *Exercise*: The types of crime and how they take place across San Francisco's police districts.\n",
    ">  \n",
    ">  * So now we'll be combining information about `PdDistrict` and `Category` to explore differences between SF's neighborhoods. First, simply list the names of SF's 10 police districts.\n",
    ">  * Which has the most crimes? Which has the most focus crimes?\n",
    ">  * Next, we want to generate a slightly more complicated graphic. I'm interested to know if there are certain crimes that happen much more in certain neighborhoods than what's typical. Below I describe how to get that plot going:\n",
    ">    - First, we need to calculate the relative probabilities of seeing each type of crime in the dataset as a whole. That's simply a normalized version of [this plot](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/CrimeOccurrencesByCategory.png). Let's call it `P(crime)`.\n",
    ">    - Next, we calculate that same probability distribution _but for each PD district_, let's call that `P(crime|district)`.\n",
    ">    - Now we look at the ratio `P(crime|district)/P(crime)`. That ratio is equal to 1 if the crime occurs at the same level within a district as in the city as a whole. If it's greater than one, it means that the crime occurs _more frequently_ within that district. If it's smaller than one, it means that the crime is _rarer within the district in question_ than in the city as a whole.\n",
    ">    - For each district plot these ratios for the 14 focus crimes. My plot looks like this\n",
    "> ![Histograms](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/conditional.png \"histograms\")\n",
    ">    - Comment on the top crimes in _Tenderloin_, _Mission_, and _Richmond_. Does this fit with the impression you get of these neighborhoods on Wikipedia?\n",
    "\n",
    "**Comment**. Notice how much awesome data science (i.e. learning about interesting real-world crime patterns) we can get out by simply counting and plotting (and looking at ratios). Pretty great, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_vis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c25779d5e3bb5cd630ca71c8f3823103a3e1cd9e66a41a661e6c731678f6e9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
